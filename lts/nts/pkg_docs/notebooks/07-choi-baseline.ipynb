{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# SegEval evaluation package\n",
    "# Add mask, add class weights?\n",
    "\n",
    "# Argparse\n",
    "# Baselines: Random (âœ“), Hearst, Choi, GraphSeg\n",
    "# Linear Regression, Louvaine\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from loader import flatten, crawl_directory, sent_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = read_document('../data/wiki_50/test/28187', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsections = document.reravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '\\n\\n\\t'.join([' '.join(s.tokens) for s in subsections[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19th century'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[430:442]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_PARAGRAPH = 100\n",
    "pattern = re.compile(\"[ \\t\\r\\f\\v]*\\n[ \\t\\r\\f\\v]*\\n[ \\t\\r\\f\\v]*\")\n",
    "matches = pattern.finditer(text)\n",
    "\n",
    "last_break = 0\n",
    "pbreaks = [0]\n",
    "for pb in matches:\n",
    "    if pb.start()-last_break < MIN_PARAGRAPH:\n",
    "        continue\n",
    "    else:\n",
    "        pbreaks.append(pb.start())\n",
    "        last_break = pb.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 182, 442]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbreaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textiler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HearstTextTiler:\n",
    "    \"\"\" Source code from nltk.tokenize. Edited slightly for efficiency, \n",
    "    compatibility with custom classes.\n",
    "    Implements Hearst TextTiling for text segmentation: \n",
    "    http://www.aclweb.org/anthology/J97-1003\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 w=100, # Pseudosentence size\n",
    "                 k=40, # Stride size\n",
    "                 stopwords=None, \n",
    "                 smoothing_width=2,\n",
    "                 demo_mode=False):\n",
    "\n",
    "        if stopwords is None:\n",
    "            from nltk.corpus import stopwords\n",
    "            stopwords = stopwords.words('english')\n",
    "        self.__dict__.update(locals())\n",
    "        del self.__dict__['self']\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.tokenize(*args, **kwargs)\n",
    "\n",
    "    def tokenize(self, document, plot = False):\n",
    "        \"\"\" Return a tokenized copy of *text*, where each \"token\" represents \n",
    "        a separate topic \"\"\"\n",
    "                \n",
    "        # TOKENIZE\n",
    "        text = '\\n\\n\\t '.join(document.text)\n",
    "#         cleaned_text = self._clean_text(text)\n",
    "        nopunct_par_breaks = self._mark_paragraph_breaks(cleaned_text)\n",
    "        tokseqs = self._divide_to_tokensequences(cleaned_text)\n",
    "\n",
    "        # Filter stopwords\n",
    "        for ts in tokseqs:\n",
    "            ts.wrdindex_list = [wi for wi in ts.wrdindex_list if wi[0] not in self.stopwords]\n",
    "\n",
    "        token_table = self._create_token_table(tokseqs, nopunct_par_breaks)\n",
    "\n",
    "        # SCORE LEXICAL\n",
    "        gap_scores = self._block_comparison(tokseqs, token_table)\n",
    "        smooth_scores = self._smooth_scores(gap_scores)\n",
    "        \n",
    "        # IDENTIFY BOUNDARIES\n",
    "        depth_scores = self._depth_scores(smooth_scores)\n",
    "        segment_boundaries = self._identify_boundaries(depth_scores)\n",
    "        paragraph_breaks = self._mark_paragraph_breaks(text)\n",
    "        \n",
    "        # Normalize boundaries so that we split at the end of original sentences in the output\n",
    "        normalized_boundaries = self._normalize_boundaries(text, segment_boundaries, paragraph_breaks)\n",
    "        \n",
    "        # PREPARE OUTPUT\n",
    "        segmented_text = []\n",
    "        prevb = 0\n",
    "\n",
    "        for b in normalized_boundaries: # take boundaries \n",
    "            if b == 0:\n",
    "                continue\n",
    "            segmented_text.append(text[prevb:b])\n",
    "            prevb = b\n",
    "\n",
    "        if prevb < len(cleaned_text): # append any text that may be remaining\n",
    "            segmented_text.append(text[prevb:])\n",
    "\n",
    "        if not segmented_text: # if no segmentations (really short or homogeneous text), return text.\n",
    "            segmented_text = [text]\n",
    "            \n",
    "        if plot: # plot if desired\n",
    "            self._plot(gap_scores, smooth_scores, depth_scores, segment_boundaries)\n",
    "        \n",
    "        segmented_text = [re.sub(\"\\n\\n\\t \", \" | \", segment) for segment in segmented_text] # clean output\n",
    "        return (gap_scores, smooth_scores, depth_scores, segment_boundaries), segmented_text\n",
    "\n",
    "    def _mark_paragraph_breaks(self, text):\n",
    "        \"\"\"Identifies indented text or line breaks as the beginning of paragraphs\"\"\"\n",
    "        \n",
    "        MIN_PARAGRAPH = 100\n",
    "        pattern = re.compile(\"[ \\t\\r\\f\\v]*\\n[ \\t\\r\\f\\v]*\\n[ \\t\\r\\f\\v]*\")\n",
    "        matches = pattern.finditer(text)\n",
    "\n",
    "        last_break = 0\n",
    "        pbreaks = [0]\n",
    "        for pb in matches:\n",
    "            if pb.start()-last_break < MIN_PARAGRAPH:\n",
    "                continue\n",
    "            else:\n",
    "                pbreaks.append(pb.start())\n",
    "                last_break = pb.start()\n",
    "\n",
    "        return pbreaks\n",
    "    \n",
    "    def _divide_to_tokensequences(self, text):\n",
    "        \"\"\" Divides the text into 'pseudosentences' of fixed size \"\"\"\n",
    "        \n",
    "        w = self.w\n",
    "        wrdindex_list = []\n",
    "        matches = re.finditer(\"\\w+\", text)\n",
    "        for match in matches:\n",
    "            wrdindex_list.append((match.group(), match.start()))\n",
    "        return [TokenSequence(i/w, wrdindex_list[i:i+w])\n",
    "                for i in range(0, len(wrdindex_list), w)]\n",
    "\n",
    "    def _create_token_table(self, token_sequences, par_breaks):\n",
    "        \"\"\" Creates a table of TokenTableFields \"\"\"\n",
    "        \n",
    "        token_table = {}\n",
    "        current_par = 0\n",
    "        current_tok_seq = 0\n",
    "        pb_iter = par_breaks.__iter__()\n",
    "        current_par_break = next(pb_iter)\n",
    "        if current_par_break == 0:\n",
    "            try:\n",
    "                current_par_break = next(pb_iter) #skip break at 0\n",
    "            except StopIteration:\n",
    "                raise ValueError(\n",
    "                    \"No paragraph breaks were found(text too short perhaps?)\"\n",
    "                    )\n",
    "        for ts in token_sequences:\n",
    "            for word, index in ts.wrdindex_list:\n",
    "                try:\n",
    "                    while index > current_par_break:\n",
    "                        current_par_break = next(pb_iter)\n",
    "                        current_par += 1\n",
    "                except StopIteration:\n",
    "                    #hit bottom\n",
    "                    pass\n",
    "\n",
    "                if word in token_table:\n",
    "                    token_table[word].total_count += 1\n",
    "\n",
    "                    if token_table[word].last_par != current_par:\n",
    "                        token_table[word].last_par = current_par\n",
    "                        token_table[word].par_count += 1\n",
    "\n",
    "                    if token_table[word].last_tok_seq != current_tok_seq:\n",
    "                        token_table[word].last_tok_seq = current_tok_seq\n",
    "                        token_table[word].ts_occurences.append([current_tok_seq,1])\n",
    "                    else:\n",
    "                        token_table[word].ts_occurences[-1][1] += 1\n",
    "                else: #new word\n",
    "                    token_table[word] = TokenTableField(first_pos=index,\n",
    "                                                        ts_occurences= \\\n",
    "                                                          [[current_tok_seq,1]],\n",
    "                                                        total_count=1,\n",
    "                                                        par_count=1,\n",
    "                                                        last_par=current_par,\n",
    "                                                        last_tok_seq= \\\n",
    "                                                          current_tok_seq)\n",
    "\n",
    "            current_tok_seq += 1\n",
    "\n",
    "        return token_table\n",
    "    \n",
    "    def _block_comparison(self, tokseqs, token_table):\n",
    "        \"Implements the block comparison method\"\n",
    "\n",
    "        gap_scores = []\n",
    "        numgaps = len(tokseqs)-1\n",
    "\n",
    "        for curr_gap in range(numgaps):\n",
    "            score_dividend, score_divisor_b1, score_divisor_b2 = 0.0, 0.0, 0.0\n",
    "            score = 0.0\n",
    "            #adjust window size for boundary conditions\n",
    "            if curr_gap < self.k-1:\n",
    "                window_size = curr_gap + 1\n",
    "            elif curr_gap > numgaps-self.k:\n",
    "                window_size = numgaps - curr_gap\n",
    "            else:\n",
    "                window_size = self.k\n",
    "\n",
    "            b1 = [ts.index\n",
    "                  for ts in tokseqs[curr_gap-window_size+1 : curr_gap+1]]\n",
    "            b2 = [ts.index\n",
    "                  for ts in tokseqs[curr_gap+1 : curr_gap+window_size+1]]\n",
    "\n",
    "            for t in token_table:\n",
    "                score_dividend += self._blk_frq(t, b1, token_table)*self._blk_frq(t, b2, token_table)\n",
    "                score_divisor_b1 += self._blk_frq(t, b1, token_table)**2\n",
    "                score_divisor_b2 += self._blk_frq(t, b2, token_table)**2\n",
    "            try:\n",
    "                score = score_dividend/((score_divisor_b1*\n",
    "                                                 score_divisor_b2)**0.5)\n",
    "            except ZeroDivisionError:\n",
    "                pass # score += 0.0\n",
    "\n",
    "            gap_scores.append(score)\n",
    "\n",
    "        return gap_scores\n",
    "    \n",
    "    def _blk_frq(self, tok, block, token_table):\n",
    "        \"\"\" Count occurrences of a token in a block \"\"\"\n",
    "        \n",
    "        ts_occs = filter(lambda o: o[0] in block,\n",
    "                         token_table[tok].ts_occurences)\n",
    "        freq = sum([tsocc[1] for tsocc in ts_occs])\n",
    "        return freq\n",
    "    \n",
    "    def _smooth_scores(self, gap_scores):\n",
    "        \"\"\" Wraps the SciPy smooth function \"\"\"\n",
    "        \n",
    "        return list(self._smooth(np.array(gap_scores[:]), window_len = self.smoothing_width+1))\n",
    "    \n",
    "    def _smooth(self, x, window_len=11, window='flat'):\n",
    "        \"\"\" Source code fom SciPy: window smoothing function \"\"\"\n",
    "        \n",
    "        if x.ndim != 1:\n",
    "            raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "        elif x.size < window_len:\n",
    "            raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
    "        elif window_len < 3:\n",
    "            return x\n",
    "        elif window not in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "            raise ValueError(\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
    "\n",
    "        s = np.r_[2*x[0]-x[window_len:1:-1],x,2*x[-1]-x[-1:-window_len:-1]]\n",
    "\n",
    "        if window == 'flat': #moving average\n",
    "            w = np.ones(window_len,'d')\n",
    "        else:\n",
    "            w = eval('np.' + window + '(window_len)')\n",
    "\n",
    "        y = np.convolve(w/w.sum(), s, mode='same')\n",
    "\n",
    "        return y[window_len-1:-window_len+1]\n",
    "    \n",
    "    def _identify_boundaries(self, depth_scores):\n",
    "        \"\"\"Identifies boundaries at the peaks of similarity score differences\"\"\"\n",
    "\n",
    "        boundaries = np.zeros(len(depth_scores))\n",
    "\n",
    "        avg = sum(depth_scores)/len(depth_scores)\n",
    "        stdev = np.std(depth_scores)\n",
    "\n",
    "        cutoff = avg-stdev/2.0\n",
    "\n",
    "        depth_tuples = sorted(zip(depth_scores, range(len(depth_scores))))\n",
    "        depth_tuples.reverse()\n",
    "        hp = list(filter(lambda x:x[0]>cutoff, depth_tuples))\n",
    "\n",
    "        for dt in hp:\n",
    "            boundaries[dt[1]] = 1\n",
    "            for dt2 in hp: #undo if there is a boundary close already\n",
    "                if dt[1] != dt2[1] and abs(dt2[1]-dt[1]) < 4 and boundaries[dt2[1]] == 1:\n",
    "                    boundaries[dt[1]] = 0\n",
    "        return boundaries\n",
    "\n",
    "    def _depth_scores(self, scores):\n",
    "        \"\"\"Calculates the depth of each gap, i.e. the average difference\n",
    "        between the left and right peaks and the gap's score\"\"\"\n",
    "\n",
    "        depth_scores = [0 for _ in scores]\n",
    "        #clip boundaries: this holds on the rule of thumb(my thumb)\n",
    "        #that a section shouldn't be smaller than at least 2\n",
    "        #pseudosentences for small texts and around 5 for larger ones.\n",
    "\n",
    "        clip = int(min(max(len(scores)/10, 2), 5))\n",
    "        index = clip\n",
    "\n",
    "        for gapscore in scores[clip:-clip]:\n",
    "            lpeak = gapscore\n",
    "            for score in scores[index::-1]:\n",
    "                if score >= lpeak:\n",
    "                    lpeak = score\n",
    "                else:\n",
    "                    break\n",
    "            rpeak = gapscore\n",
    "            for score in scores[index:]:\n",
    "                if score >= rpeak:\n",
    "                    rpeak = score\n",
    "                else:\n",
    "                    break\n",
    "            depth_scores[index] = lpeak + rpeak - 2 * gapscore\n",
    "            index += 1\n",
    "\n",
    "        return depth_scores\n",
    "    \n",
    "    def _normalize_boundaries(self, text, boundaries, paragraph_breaks):\n",
    "        \"\"\"Normalize the boundaries identified to the original text's\n",
    "        paragraph breaks\"\"\"\n",
    "\n",
    "        norm_boundaries = []\n",
    "        char_count, word_count, gaps_seen = 0, 0, 0\n",
    "        seen_word = False\n",
    "\n",
    "        for char in text:\n",
    "            char_count += 1\n",
    "            if char in \" \\t\\n\" and seen_word:\n",
    "                seen_word = False\n",
    "                word_count += 1\n",
    "            if char not in \" \\t\\n\" and not seen_word:\n",
    "                seen_word=True\n",
    "            if gaps_seen < len(boundaries) and word_count > (max(gaps_seen*self.w, self.w)):\n",
    "                if boundaries[gaps_seen] == 1:\n",
    "                    #find closest paragraph break\n",
    "                    best_fit = len(text)\n",
    "                    for br in paragraph_breaks:\n",
    "                        if best_fit > abs(br-char_count):\n",
    "                            best_fit = abs(br-char_count)\n",
    "                            bestbr = br\n",
    "                        else:\n",
    "                            break\n",
    "                    if bestbr not in norm_boundaries: #avoid duplicates\n",
    "                        norm_boundaries.append(bestbr)\n",
    "                gaps_seen += 1\n",
    "\n",
    "        return norm_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_baseline = Random(test_dir='../data/wiki_50/test')\n",
    "random_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segeval.window.pk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = [2,3,3,3,6,3]\n",
    "hypo = (2,6,4,2,4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = [0,0,0,0,2,1,1,1,0,1,0]\n",
    "hypo = [0,0,0,1,1,1,1,1,0,1,0]\n",
    "segeval.pk(ref, hypo, return_parts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(segeval.window.pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = trainer.predict_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segeval.pk(batch.labels.tolist(), out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch.labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
