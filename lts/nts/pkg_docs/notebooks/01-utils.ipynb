{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, nltk\n",
    "from cached_property import cached_property\n",
    "\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "class Document:\n",
    "    \"\"\" Contains all sentences in a Wiki article and the filename \"\"\"\n",
    "    def __init__(self, sentences, filename):\n",
    "        self.sentences = sentences\n",
    "        self.filename = filename\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Document containing %d sentences' % (len(self.sentences))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "\n",
    "class Sentence:\n",
    "    \"\"\" Contain text, tokenized text, and label of a sentence \"\"\"\n",
    "    def __init__(self, text, label):\n",
    "        self.text = text\n",
    "        self.tokens = [clean_token(t) for t in text.split()]\n",
    "        self.label = label\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '\"' + self.text + '\"'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "\n",
    "class LazyVectors:\n",
    "    \"\"\"Load only those vectors from GloVE that are in the vocab.\"\"\"\n",
    "    \n",
    "    unk_idx = 1\n",
    "\n",
    "    def __init__(self, name='glove.840B.300d.txt'):\n",
    "        \"\"\" Requires the glove vectors to be in a folder named .vector_cache\n",
    "        \n",
    "        In Bash/Terminal from directory this class is in:\n",
    "        >> mkdir .vector_cache\n",
    "        >> mv glove.840B.300d.txt .vector_cache/glove.840B.300d.txt\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.set_vocab()\n",
    "\n",
    "    @cached_property\n",
    "    def loader(self):\n",
    "        return Vectors(self.name)\n",
    "    \n",
    "    def get_vocab(self, filename='vocabulary.txt'):\n",
    "        with open(filename, 'r') as f:\n",
    "            vocab = f.read().split(',')\n",
    "        return vocab\n",
    "    \n",
    "    def set_vocab(self):\n",
    "        \"\"\"Set corpus vocab \"\"\"\n",
    "        # Intersect with model vocab.\n",
    "        self.vocab = [v for v in self.get_vocab() if v in self.loader.stoi]\n",
    "\n",
    "        # Map string -> intersected index.\n",
    "        self._stoi = {s: i for i, s in enumerate(self.vocab)}\n",
    "\n",
    "    def weights(self):\n",
    "        \"\"\"Build weights tensor for embedding layer \"\"\"\n",
    "        # Select vectors for vocab words.\n",
    "        weights = torch.stack([\n",
    "            self.loader.vectors[self.loader.stoi[s]]\n",
    "            for s in self.vocab\n",
    "        ])\n",
    "\n",
    "        # Padding + UNK zeros rows.\n",
    "        return torch.cat([\n",
    "            torch.zeros((2, self.loader.dim)),\n",
    "            weights,\n",
    "        ])\n",
    "\n",
    "    def stoi(self, s):\n",
    "        \"\"\"Map string -> embedding index.\n",
    "        \"\"\"\n",
    "        idx = self._stoi.get(s)\n",
    "        return idx + 2 if idx else self.unk_idx\n",
    "\n",
    "\n",
    "def crawl_directory(dirname):\n",
    "    \"\"\" Walk a nested directory to get all filename ending in a pattern \"\"\"\n",
    "    filenames = []\n",
    "    for path, subdirs, files in os.walk(dirname):\n",
    "        for name in files:\n",
    "            if not name.endswith('.DS_Store'):\n",
    "                yield os.path.join(path, name)\n",
    "\n",
    "def read_document(filename, minlen=0):\n",
    "    \"\"\" Read in a Wiki-727 file \"\"\"\n",
    "    doc, subsection = [], ''\n",
    "    with open(filename) as f:\n",
    "        for line in f.readlines()[1:]:\n",
    "            if line.startswith('========'):\n",
    "                doc.append(subsection)\n",
    "                subsection = ''\n",
    "            else:\n",
    "                subsection += ' ' + line[:-1] # Exclude \\n \n",
    "        doc.append(subsection)\n",
    "        \n",
    "    doc = flatten([sent_tokenizer.tokenize(d.strip())\n",
    "                for d in doc if len(d) > minlen]) # Exclude empty subsections\n",
    "    \n",
    "    labels = doc_to_labels(doc)\n",
    "    \n",
    "    document = Document([Sentence(text, label) for text, label in zip(doc, labels)], \n",
    "                         filename)\n",
    "        \n",
    "    return document\n",
    "        \n",
    "def doc_to_labels(document):\n",
    "    \"\"\" Convert Wiki-727 file to labels \n",
    "    (last sentence of a subsection is 1, otherwise 0) \"\"\"\n",
    "    return flatten([(len(doc)-1)*[0] + [1] for doc in document])\n",
    "\n",
    "def clean_token(token):\n",
    "    \"\"\"  Remove everything but whitespace, the alphabet; \n",
    "    separate apostrophes for stopwords \"\"\"\n",
    "    if token.isdigit():\n",
    "        token = '#NUM'\n",
    "    else:\n",
    "        token = re.sub(r\"[^a-z0-9\\s]\", '', token.lower())\n",
    "        token = re.sub(r\"[']+\", ' ', token)\n",
    "    return token\n",
    "\n",
    "def flatten(alist):\n",
    "    \"\"\" Flatten a list of lists into one list \"\"\"\n",
    "    return [item for sublist in alist for item in sublist]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
