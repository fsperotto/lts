{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cddd495c38f94906aec61edc079f8283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 0.499121 | Label 1: 0.500880\n",
      "Step: 1 | Loss: 0.157997 | Num. sents: 10761 | Segs correct: 756 / 1306 | Texts correct: 3894 / 9455\n",
      "Label 0: 0.497782 | Label 1: 0.502217\n",
      "Step: 2 | Loss: 0.159785 | Num. sents: 8610 | Segs correct: 775 / 1059 | Texts correct: 2344 / 7551\n",
      "Label 0: 0.496486 | Label 1: 0.503514\n",
      "Step: 3 | Loss: 0.154951 | Num. sents: 9865 | Segs correct: 917 / 1174 | Texts correct: 1984 / 8691\n",
      "Label 0: 0.495092 | Label 1: 0.504908\n",
      "Step: 4 | Loss: 0.145292 | Num. sents: 11265 | Segs correct: 1088 / 1252 | Texts correct: 1555 / 10013\n",
      "Label 0: 0.493807 | Label 1: 0.506193\n",
      "Step: 5 | Loss: 0.153638 | Num. sents: 10273 | Segs correct: 1108 / 1213 | Texts correct: 1019 / 9060\n",
      "Label 0: 0.492255 | Label 1: 0.507745\n",
      "Step: 6 | Loss: 0.154408 | Num. sents: 9568 | Segs correct: 1081 / 1136 | Texts correct: 633 / 8432\n",
      "Label 0: 0.490948 | Label 1: 0.509052\n",
      "Step: 7 | Loss: 0.151097 | Num. sents: 10481 | Segs correct: 1178 / 1216 | Texts correct: 577 / 9265\n",
      "Label 0: 0.489256 | Label 1: 0.510744\n",
      "Step: 8 | Loss: 0.146033 | Num. sents: 10149 | Segs correct: 1102 / 1136 | Texts correct: 353 / 9013\n",
      "Label 0: 0.488341 | Label 1: 0.511659\n",
      "Step: 9 | Loss: 0.154894 | Num. sents: 9654 | Segs correct: 1127 / 1151 | Texts correct: 272 / 8503\n",
      "Label 0: 0.486888 | Label 1: 0.513112\n",
      "Step: 10 | Loss: 0.160282 | Num. sents: 9658 | Segs correct: 1169 / 1195 | Texts correct: 245 / 8463\n",
      "Label 0: 0.485821 | Label 1: 0.514179\n",
      "Step: 11 | Loss: 0.159529 | Num. sents: 9904 | Segs correct: 1197 / 1219 | Texts correct: 249 / 8685\n",
      "Label 0: 0.485862 | Label 1: 0.514137\n",
      "Step: 12 | Loss: 0.140360 | Num. sents: 11139 | Segs correct: 1174 / 1198 | Texts correct: 334 / 9941\n",
      "Label 0: 0.484522 | Label 1: 0.515479\n",
      "Step: 13 | Loss: 0.158870 | Num. sents: 9564 | Segs correct: 1153 / 1174 | Texts correct: 257 / 8390\n",
      "Label 0: 0.483500 | Label 1: 0.516500\n",
      "Step: 14 | Loss: 0.153485 | Num. sents: 10692 | Segs correct: 1247 / 1265 | Texts correct: 252 / 9427\n",
      "Label 0: 0.482813 | Label 1: 0.517187\n",
      "Step: 15 | Loss: 0.150405 | Num. sents: 10654 | Segs correct: 1217 / 1234 | Texts correct: 268 / 9420\n",
      "Label 0: 0.481881 | Label 1: 0.518120\n",
      "Step: 16 | Loss: 0.150691 | Num. sents: 10201 | Segs correct: 1168 / 1184 | Texts correct: 242 / 9017\n",
      "Label 0: 0.480833 | Label 1: 0.519167\n",
      "Step: 17 | Loss: 0.154375 | Num. sents: 10873 | Segs correct: 1274 / 1295 | Texts correct: 227 / 9578\n",
      "Label 0: 0.480797 | Label 1: 0.519203\n",
      "Step: 18 | Loss: 0.151411 | Num. sents: 10176 | Segs correct: 1170 / 1187 | Texts correct: 262 / 8989\n",
      "Label 0: 0.479928 | Label 1: 0.520073\n",
      "Step: 19 | Loss: 0.149938 | Num. sents: 11010 | Segs correct: 1251 / 1271 | Texts correct: 231 / 9739\n",
      "Label 0: 0.478916 | Label 1: 0.521084\n",
      "Step: 20 | Loss: 0.160331 | Num. sents: 10143 | Segs correct: 1242 / 1260 | Texts correct: 244 / 8883\n",
      "Label 0: 0.479130 | Label 1: 0.520870\n",
      "Step: 21 | Loss: 0.148839 | Num. sents: 10581 | Segs correct: 1186 / 1214 | Texts correct: 301 / 9367\n",
      "Label 0: 0.479541 | Label 1: 0.520459\n",
      "Step: 22 | Loss: 0.136764 | Num. sents: 10682 | Segs correct: 1090 / 1119 | Texts correct: 410 / 9563\n",
      "Label 0: 0.477904 | Label 1: 0.522096\n",
      "Step: 23 | Loss: 0.155197 | Num. sents: 9664 | Segs correct: 1135 / 1159 | Texts correct: 287 / 8505\n",
      "Label 0: 0.478855 | Label 1: 0.521145\n",
      "Step: 24 | Loss: 0.144193 | Num. sents: 10807 | Segs correct: 1169 / 1199 | Texts correct: 433 / 9608\n",
      "Label 0: 0.478582 | Label 1: 0.521418\n",
      "Step: 25 | Loss: 0.153860 | Num. sents: 9877 | Segs correct: 1142 / 1175 | Texts correct: 372 / 8702\n",
      "Label 0: 0.478262 | Label 1: 0.521737\n",
      "Step: 26 | Loss: 0.153900 | Num. sents: 10131 | Segs correct: 1177 / 1205 | Texts correct: 390 / 8926\n",
      "Label 0: 0.478437 | Label 1: 0.521563\n",
      "Step: 27 | Loss: 0.154486 | Num. sents: 9674 | Segs correct: 1124 / 1157 | Texts correct: 452 / 8517\n",
      "Label 0: 0.478410 | Label 1: 0.521591\n",
      "Step: 28 | Loss: 0.141310 | Num. sents: 10948 | Segs correct: 1159 / 1189 | Texts correct: 563 / 9759\n",
      "Label 0: 0.478074 | Label 1: 0.521927\n",
      "Step: 29 | Loss: 0.153970 | Num. sents: 9425 | Segs correct: 1099 / 1125 | Texts correct: 598 / 8300\n",
      "Label 0: 0.479825 | Label 1: 0.520175\n",
      "Step: 30 | Loss: 0.138285 | Num. sents: 10310 | Segs correct: 1050 / 1096 | Texts correct: 719 / 9214\n",
      "Label 0: 0.478266 | Label 1: 0.521734\n",
      "Step: 31 | Loss: 0.150021 | Num. sents: 9890 | Segs correct: 1113 / 1147 | Texts correct: 619 / 8743\n",
      "Label 0: 0.478766 | Label 1: 0.521234\n",
      "Step: 32 | Loss: 0.138759 | Num. sents: 10846 | Segs correct: 1118 / 1158 | Texts correct: 852 / 9688\n",
      "Label 0: 0.478561 | Label 1: 0.521439\n",
      "Step: 33 | Loss: 0.154638 | Num. sents: 9732 | Segs correct: 1115 / 1167 | Texts correct: 726 / 8565\n",
      "Label 0: 0.478894 | Label 1: 0.521105\n",
      "Step: 34 | Loss: 0.139172 | Num. sents: 11055 | Segs correct: 1124 / 1183 | Texts correct: 1006 / 9872\n",
      "Label 0: 0.480289 | Label 1: 0.519711\n",
      "Step: 35 | Loss: 0.150714 | Num. sents: 10282 | Segs correct: 1124 / 1199 | Texts correct: 1102 / 9083\n",
      "Label 0: 0.479855 | Label 1: 0.520145\n",
      "Step: 36 | Loss: 0.163224 | Num. sents: 9124 | Segs correct: 1107 / 1159 | Texts correct: 888 / 7965\n",
      "Label 0: 0.479098 | Label 1: 0.520901\n",
      "Step: 37 | Loss: 0.166023 | Num. sents: 8805 | Segs correct: 1077 / 1140 | Texts correct: 844 / 7665\n",
      "Label 0: 0.480577 | Label 1: 0.519423\n",
      "Step: 38 | Loss: 0.144596 | Num. sents: 10996 | Segs correct: 1135 / 1228 | Texts correct: 1395 / 9768\n",
      "Label 0: 0.481577 | Label 1: 0.518423\n",
      "Step: 39 | Loss: 0.149511 | Num. sents: 9828 | Segs correct: 1052 / 1138 | Texts correct: 1322 / 8690\n",
      "Label 0: 0.481389 | Label 1: 0.518611\n",
      "Step: 40 | Loss: 0.156186 | Num. sents: 9043 | Segs correct: 996 / 1097 | Texts correct: 1259 / 7946\n",
      "Label 0: 0.480890 | Label 1: 0.519110\n",
      "Step: 41 | Loss: 0.155348 | Num. sents: 9236 | Segs correct: 1015 / 1115 | Texts correct: 1222 / 8121\n",
      "Label 0: 0.482225 | Label 1: 0.517775\n",
      "Step: 42 | Loss: 0.145119 | Num. sents: 10117 | Segs correct: 1030 / 1135 | Texts correct: 1547 / 8982\n",
      "Label 0: 0.482141 | Label 1: 0.517859\n",
      "Step: 43 | Loss: 0.147611 | Num. sents: 10366 | Segs correct: 1064 / 1183 | Texts correct: 1589 / 9183\n",
      "Label 0: 0.481265 | Label 1: 0.518736\n",
      "Step: 44 | Loss: 0.146919 | Num. sents: 11000 | Segs correct: 1132 / 1250 | Texts correct: 1587 / 9750\n",
      "Label 0: 0.479763 | Label 1: 0.520237\n",
      "Step: 45 | Loss: 0.162052 | Num. sents: 9927 | Segs correct: 1147 / 1253 | Texts correct: 1382 / 8674\n",
      "Label 0: 0.483515 | Label 1: 0.516484\n",
      "Step: 46 | Loss: 0.144743 | Num. sents: 9516 | Segs correct: 961 / 1070 | Texts correct: 1856 / 8446\n",
      "Label 0: 0.480122 | Label 1: 0.519878\n",
      "Step: 47 | Loss: 0.148311 | Num. sents: 11505 | Segs correct: 1197 / 1320 | Texts correct: 1686 / 10185\n",
      "Label 0: 0.482470 | Label 1: 0.517529\n",
      "Step: 48 | Loss: 0.154232 | Num. sents: 9920 | Segs correct: 1073 / 1191 | Texts correct: 1787 / 8729\n",
      "Label 0: 0.480803 | Label 1: 0.519196\n",
      "Step: 49 | Loss: 0.154971 | Num. sents: 9273 | Segs correct: 1015 / 1118 | Texts correct: 1519 / 8155\n",
      "Label 0: 0.481765 | Label 1: 0.518235\n",
      "Step: 50 | Loss: 0.166841 | Num. sents: 9005 | Segs correct: 1040 / 1173 | Texts correct: 1632 / 7832\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c59c3213e523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m trainer.train(num_epochs=100, \n\u001b[1;32m     78\u001b[0m               \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m               val_ckpt=1)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/text-seg/src/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, *args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;34m\"\"\" Train a model \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ckpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/text-seg/src/trainer.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, epoch, steps, val_ckpt, visualize)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;31m# Compute a train batch, backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegs_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_segs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/text-seg/src/trainer.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Sample a batch of documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_and_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Get predictions for each document in the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/text-seg/src/loader.py\u001b[0m in \u001b[0;36msample_and_batch\u001b[0;34m(TRAIN, *args)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;34m\"\"\" Sample some directory path and Batch the documents \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_nested_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mread_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/text-seg/src/loader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;34m\"\"\" Sample some directory path and Batch the documents \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_nested_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mread_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/text-seg/src/loader.py\u001b[0m in \u001b[0;36mread_document\u001b[0;34m(filename, TRAIN, minlen)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# This '========' indicates a new subsection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'========'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/text-seg/env/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m-> 1241\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/text-seg/env/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \"\"\"\n\u001b[0;32m-> 1291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/text-seg/env/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \"\"\"\n\u001b[0;32m-> 1291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/text-seg/env/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/text-seg/env/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \"\"\"\n\u001b[1;32m   1321\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/text-seg/env/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/text-seg/env/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.trainer import *\n",
    "\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, output_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings = BagOfWords()\n",
    "        self.linear = nn.Linear(self.embeddings.vocab_size, output_dim)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        embedded = self.embeddings(batch)\n",
    "        preds = self.linear(embedded)\n",
    "        # Sigmoid?\n",
    "        return preds.squeeze()\n",
    "    \n",
    "\n",
    "class BagOfWords:\n",
    "    \n",
    "    unk_idx = 0\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.set_vocab()\n",
    "        \n",
    "    def __call__(self, *args):\n",
    "        return self.featurize(*args)\n",
    "    \n",
    "    def featurize(self, batch):\n",
    "        \"\"\" Turn batch or document into discrete, binarized bag of words \"\"\"\n",
    "        sentences = [s.tokens for s in batch.sents]\n",
    "        token_idx = [[self.stoi(t) for t in tokenized] \n",
    "                      for tokenized in sentences]\n",
    "        \n",
    "        featurized = torch.zeros((len(token_idx), self.vocab_size))\n",
    "        for sent_idx, counts in enumerate(token_idx):\n",
    "            featurized[sent_idx, counts] += 1\n",
    "        \n",
    "        return to_var(featurized)\n",
    "    \n",
    "    def set_vocab(self):\n",
    "        \"\"\"Set corpus vocab \"\"\"\n",
    "        # Map string to intersected index.\n",
    "        self._stoi = {s: i for i, s in enumerate(self.get_vocab())}\n",
    "        \n",
    "        # Vocab size, plus one for UNK tokens (idx: 0)\n",
    "        self.vocab_size = len(self._stoi.keys()) + 1 \n",
    "        \n",
    "    def get_vocab(self, filename='../src/vocabulary.txt'):\n",
    "        \"\"\" Read in vocabulary (top 30K words, covers ~93.5% of all tokens) \"\"\" \n",
    "        with open(filename, 'r') as f:\n",
    "            vocab = f.read().split(',')\n",
    "        return vocab\n",
    "        \n",
    "    def stoi(self, s):\n",
    "        \"\"\" String to index (s to i) for embedding lookup \"\"\"\n",
    "        idx = self._stoi.get(s)\n",
    "        return idx + 1 if idx else self.unk_idx\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  train_dir='../data/wiki_727/train',\n",
    "                  val_dir='../data/wiki_50/test',\n",
    "                  test_dir=None,\n",
    "                  batch_size=256,\n",
    "                  lr=5e-4)\n",
    "\n",
    "trainer.train(num_epochs=100, \n",
    "              steps=100,\n",
    "              val_ckpt=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
