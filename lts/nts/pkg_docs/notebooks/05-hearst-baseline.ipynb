{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:09<00:00,  5.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'w_pk': Decimal('0.2793010546301708582865006732'),\n",
       " 'w_wd': Decimal('0.2550243673220345814983931182'),\n",
       " 'w_ss': Decimal('0.9949244432802800329880786732'),\n",
       " 'w_bs': Decimal('0.256010946907498631636562671'),\n",
       " 'w_precision': Decimal('0.256010946907498631636562671'),\n",
       " 'w_recall': Decimal('0.256010946907498631636562671'),\n",
       " 'w_f1': Decimal('0.3486022644022644022644022644'),\n",
       " 's_pk': Decimal('0.310357687801236037926014378'),\n",
       " 's_wd': Decimal('0.2567737429493450753766671978'),\n",
       " 's_ss': Decimal('0.8914301871659809530707930202'),\n",
       " 's_bs': Decimal('0.256010946907498631636562671'),\n",
       " 's_precision': Decimal('0.256010946907498631636562671'),\n",
       " 's_recall': Decimal('0.256010946907498631636562671'),\n",
       " 's_f1': Decimal('0.3486022644022644022644022644')}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.metrics import Metrics, avg_dicts\n",
    "from src.loader import *\n",
    "\n",
    "\n",
    "class HearstTextTiler:\n",
    "    \"\"\" Source code adapted from Stanford nltk.tokenize. \n",
    "    Edited slightly for efficiency and compatibility.\n",
    "\n",
    "    http://www.aclweb.org/anthology/J97-1003\n",
    "    \"\"\"\n",
    "    \n",
    "    evalu = Metrics()\n",
    "    \n",
    "    def __init__(self,\n",
    "                 w=100, # Pseudosentence size\n",
    "                 k=40, # Stride size\n",
    "                 stopwords=None, \n",
    "                 smoothing_width=2,\n",
    "                 demo_mode=False):\n",
    "\n",
    "        self.__dict__.update(locals())\n",
    "        del self.__dict__['self']\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.segment(*args, **kwargs)\n",
    "    \n",
    "    def validate(self, dirname):\n",
    "        \"\"\" Evaluate SegEval performance on a directory \"\"\"\n",
    "        \n",
    "        # Read in documents\n",
    "        documents = [read_document(f, False) for f in crawl_directory(dirname)]\n",
    "        dictionaries = []\n",
    "        for document in tqdm(documents):\n",
    "            \n",
    "            # Segment document\n",
    "            _, segmented_text = self.segment(document)\n",
    "            \n",
    "            # Regroup into segmented paragraphs\n",
    "            segments = [[s for s in seg.strip().split('|') if s] for seg in segmented_text]\n",
    "            \n",
    "            # Get counts in tokens per sentence\n",
    "            counts = [len(s.split()) for seg in segments for s in seg]\n",
    "\n",
    "            # Convert counts to labels, PseudoBatch for evaluation\n",
    "            preds = counts_to_labels([len(s) for s in segments])\n",
    "            batch = PseudoBatch(counts, document.labels)\n",
    "\n",
    "            # Evaluate\n",
    "            metrics_dict = self.evalu(batch, preds)\n",
    "            dictionaries.append(metrics_dict)\n",
    "            \n",
    "        # Average performance\n",
    "        merged = avg_dicts(dictionaries)\n",
    "    \n",
    "        return merged\n",
    "    \n",
    "    def segment(self, document, plot = False):\n",
    "        \"\"\" Return a tokenized copy of *text*, where each \"token\" represents \n",
    "        a separate topic \"\"\"\n",
    "                \n",
    "        # TOKENIZE\n",
    "        subsections = document.reravel()\n",
    "        text = '=== '.join([' |'.join([' '.join(s.tokens) for s in sub]) \n",
    "                           for sub in subsections])\n",
    "        \n",
    "        nopunct_par_breaks = self._mark_paragraph_breaks(text)\n",
    "        tokseqs = self._divide_to_tokensequences(text)\n",
    "\n",
    "        # Filter stopwords\n",
    "        if self.stopwords:\n",
    "            for ts in tokseqs:\n",
    "                ts.wrdindex_list = [wi for wi in ts.wrdindex_list \n",
    "                                    if wi[0] not in self.stopwords]\n",
    "\n",
    "        token_table = self._create_token_table(tokseqs, nopunct_par_breaks)\n",
    "\n",
    "        # SCORE LEXICAL\n",
    "        gap_scores = self._block_comparison(tokseqs, token_table)\n",
    "        smooth_scores = self._smooth_scores(gap_scores)\n",
    "        \n",
    "        # IDENTIFY BOUNDARIES\n",
    "        depth_scores = self._depth_scores(smooth_scores)\n",
    "        segment_boundaries = self._identify_boundaries(depth_scores)\n",
    "        paragraph_breaks = self._mark_paragraph_breaks(text)\n",
    "        \n",
    "        # Normalize boundaries so that we split at the end of original sentences in the output\n",
    "        normalized_boundaries = self._normalize_boundaries(text, segment_boundaries, paragraph_breaks)\n",
    "        \n",
    "        # PREPARE OUTPUT\n",
    "        segmented_text = []\n",
    "        prevb = 0\n",
    "\n",
    "        for b in normalized_boundaries: # take boundaries \n",
    "            if b == 0:\n",
    "                continue\n",
    "            segmented_text.append(text[prevb:b])\n",
    "            prevb = b\n",
    "\n",
    "        if prevb < len(text): # append any text that may be remaining\n",
    "            segmented_text.append(text[prevb:])\n",
    "\n",
    "        if not segmented_text: # if no segmentations (really short or homogeneous text), return text.\n",
    "            segmented_text = [text]\n",
    "            \n",
    "        if plot: # plot if desired\n",
    "            self._plot(gap_scores, smooth_scores, depth_scores, segment_boundaries)\n",
    "        \n",
    "        segmented_text = [re.sub(\"=== \", \" |\", segment.strip()) for segment in segmented_text] # clean output\n",
    "        return (gap_scores, smooth_scores, depth_scores, segment_boundaries), segmented_text\n",
    "\n",
    "    def _mark_paragraph_breaks(self, text):\n",
    "        \"\"\"Identifies indented text or line breaks as the beginning of paragraphs\"\"\"\n",
    "        \n",
    "        MIN_PARAGRAPH = 1\n",
    "        pattern = re.compile(\"=== \")\n",
    "        matches = pattern.finditer(text)\n",
    "\n",
    "        last_break = 0\n",
    "        pbreaks = [0]\n",
    "        for pb in matches:\n",
    "            if pb.start()-last_break < MIN_PARAGRAPH:\n",
    "                continue\n",
    "            else:\n",
    "                pbreaks.append(pb.start())\n",
    "                last_break = pb.start()\n",
    "\n",
    "        return pbreaks\n",
    "    \n",
    "    def _divide_to_tokensequences(self, text):\n",
    "        \"\"\" Divides the text into 'pseudosentences' of fixed size \"\"\"\n",
    "        \n",
    "        w = self.w\n",
    "        wrdindex_list = []\n",
    "        matches = re.finditer(\"\\w+\", text)\n",
    "        for match in matches:\n",
    "            wrdindex_list.append((match.group(), match.start()))\n",
    "            \n",
    "        return [TokenSequence(i/w, wrdindex_list[i:i+w])\n",
    "                for i in range(0, len(wrdindex_list), w)]\n",
    "\n",
    "    def _create_token_table(self, token_sequences, par_breaks):\n",
    "        \"\"\" Creates a table of TokenTableFields \"\"\"\n",
    "        \n",
    "        token_table = {}\n",
    "        current_par = 0\n",
    "        current_tok_seq = 0\n",
    "        pb_iter = par_breaks.__iter__()\n",
    "        current_par_break = next(pb_iter)\n",
    "        if current_par_break == 0:\n",
    "            try:\n",
    "                current_par_break = next(pb_iter) #skip break at 0\n",
    "            except StopIteration:\n",
    "                raise ValueError(\n",
    "                    \"No paragraph breaks were found(text too short perhaps?)\"\n",
    "                    )\n",
    "        for ts in token_sequences:\n",
    "            for word, index in ts.wrdindex_list:\n",
    "                try:\n",
    "                    while index > current_par_break:\n",
    "                        current_par_break = next(pb_iter)\n",
    "                        current_par += 1\n",
    "                except StopIteration:\n",
    "                    #hit bottom\n",
    "                    pass\n",
    "\n",
    "                if word in token_table:\n",
    "                    token_table[word].total_count += 1\n",
    "\n",
    "                    if token_table[word].last_par != current_par:\n",
    "                        token_table[word].last_par = current_par\n",
    "                        token_table[word].par_count += 1\n",
    "\n",
    "                    if token_table[word].last_tok_seq != current_tok_seq:\n",
    "                        token_table[word].last_tok_seq = current_tok_seq\n",
    "                        token_table[word].ts_occurences.append([current_tok_seq,1])\n",
    "                    else:\n",
    "                        token_table[word].ts_occurences[-1][1] += 1\n",
    "                else: #new word\n",
    "                    token_table[word] = TokenTableField(first_pos=index,\n",
    "                                                        ts_occurences= \\\n",
    "                                                          [[current_tok_seq,1]],\n",
    "                                                        total_count=1,\n",
    "                                                        par_count=1,\n",
    "                                                        last_par=current_par,\n",
    "                                                        last_tok_seq= \\\n",
    "                                                          current_tok_seq)\n",
    "\n",
    "            current_tok_seq += 1\n",
    "\n",
    "        return token_table\n",
    "    \n",
    "    def _block_comparison(self, tokseqs, token_table):\n",
    "        \"Implements the block comparison method\"\n",
    "\n",
    "        gap_scores = []\n",
    "        numgaps = len(tokseqs)-1\n",
    "\n",
    "        for curr_gap in range(numgaps):\n",
    "            score_dividend, score_divisor_b1, score_divisor_b2 = 0.0, 0.0, 0.0\n",
    "            score = 0.0\n",
    "            #adjust window size for boundary conditions\n",
    "            if curr_gap < self.k-1:\n",
    "                window_size = curr_gap + 1\n",
    "            elif curr_gap > numgaps-self.k:\n",
    "                window_size = numgaps - curr_gap\n",
    "            else:\n",
    "                window_size = self.k\n",
    "\n",
    "            b1 = [ts.index\n",
    "                  for ts in tokseqs[curr_gap-window_size+1 : curr_gap+1]]\n",
    "            b2 = [ts.index\n",
    "                  for ts in tokseqs[curr_gap+1 : curr_gap+window_size+1]]\n",
    "\n",
    "            for t in token_table:\n",
    "                score_dividend += self._blk_frq(t, b1, token_table)*self._blk_frq(t, b2, token_table)\n",
    "                score_divisor_b1 += self._blk_frq(t, b1, token_table)**2\n",
    "                score_divisor_b2 += self._blk_frq(t, b2, token_table)**2\n",
    "            try:\n",
    "                score = score_dividend/((score_divisor_b1*\n",
    "                                                 score_divisor_b2)**0.5)\n",
    "            except ZeroDivisionError:\n",
    "                pass\n",
    "\n",
    "            gap_scores.append(score)\n",
    "\n",
    "        return gap_scores\n",
    "    \n",
    "    def _blk_frq(self, tok, block, token_table):\n",
    "        \"\"\" Count occurrences of a token in a block \"\"\"\n",
    "        \n",
    "        ts_occs = filter(lambda o: o[0] in block,\n",
    "                         token_table[tok].ts_occurences)\n",
    "        freq = sum([tsocc[1] for tsocc in ts_occs])\n",
    "        return freq\n",
    "    \n",
    "    def _smooth_scores(self, gap_scores):\n",
    "        \"\"\" Wraps the SciPy smooth function \"\"\"\n",
    "        \n",
    "        return list(self._smooth(np.array(gap_scores[:]), window_len = self.smoothing_width+1))\n",
    "    \n",
    "    def _smooth(self, x, window_len=11, window='flat'):\n",
    "        \"\"\" Source code fom SciPy: window smoothing function \"\"\"\n",
    "        \n",
    "        if x.ndim != 1:\n",
    "            raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "        elif (window_len < 3) or (x.size < window_len):\n",
    "            return x\n",
    "\n",
    "        s = np.r_[2*x[0]-x[window_len:1:-1],x,2*x[-1]-x[-1:-window_len:-1]]\n",
    "\n",
    "        w = np.ones(window_len, 'd')\n",
    "\n",
    "        y = np.convolve(w/w.sum(), s, mode='same')\n",
    "\n",
    "        return y[window_len-1:-window_len+1]\n",
    "    \n",
    "    def _identify_boundaries(self, depth_scores):\n",
    "        \"\"\"Identifies boundaries at the peaks of similarity score differences\"\"\"\n",
    "\n",
    "        boundaries = np.zeros(len(depth_scores))\n",
    "\n",
    "        avg = sum(depth_scores)/len(depth_scores)\n",
    "        stdev = np.std(depth_scores)\n",
    "\n",
    "        cutoff = avg-stdev/2.0\n",
    "\n",
    "        depth_tuples = sorted(zip(depth_scores, range(len(depth_scores))))\n",
    "        depth_tuples.reverse()\n",
    "        hp = list(filter(lambda x:x[0]>cutoff, depth_tuples))\n",
    "\n",
    "        for dt in hp:\n",
    "            boundaries[dt[1]] = 1\n",
    "            for dt2 in hp: # undo if there is a boundary close already\n",
    "                if dt[1] != dt2[1] and abs(dt2[1]-dt[1]) < 4 and boundaries[dt2[1]] == 1:\n",
    "                    boundaries[dt[1]] = 0\n",
    "        \n",
    "        return boundaries\n",
    "\n",
    "    def _depth_scores(self, scores):\n",
    "        \"\"\"Calculates the depth of each gap, i.e. the average difference\n",
    "        between the left and right peaks and the gap's score\"\"\"\n",
    "\n",
    "        depth_scores = [0 for _ in scores]\n",
    "        # clip boundaries: this holds on the rule of thumb(my thumb)\n",
    "        # that a section shouldn't be smaller than at least 2\n",
    "        # pseudosentences for small texts and around 5 for larger ones.\n",
    "\n",
    "        clip = int(min(max(len(scores)/10, 2), 5))\n",
    "        index = clip\n",
    "\n",
    "        for gapscore in scores[clip:-clip]:\n",
    "            lpeak = gapscore\n",
    "            for score in scores[index::-1]:\n",
    "                if score >= lpeak:\n",
    "                    lpeak = score\n",
    "                else:\n",
    "                    break\n",
    "            rpeak = gapscore\n",
    "            for score in scores[index:]:\n",
    "                if score >= rpeak:\n",
    "                    rpeak = score\n",
    "                else:\n",
    "                    break\n",
    "            depth_scores[index] = lpeak + rpeak - 2 * gapscore\n",
    "            index += 1\n",
    "\n",
    "        return depth_scores\n",
    "    \n",
    "    def _normalize_boundaries(self, text, boundaries, paragraph_breaks):\n",
    "        \"\"\"Normalize the boundaries identified to the original text's\n",
    "        paragraph breaks\"\"\"\n",
    "\n",
    "        norm_boundaries = []\n",
    "        char_count, word_count, gaps_seen = 0, 0, 0\n",
    "        seen_word = False\n",
    "\n",
    "        for char in text:\n",
    "            char_count += 1\n",
    "            if char in \" ===\" and seen_word:\n",
    "                seen_word = False\n",
    "                word_count += 1\n",
    "            if char not in \" ===\" and not seen_word:\n",
    "                seen_word = True\n",
    "            if gaps_seen < len(boundaries) and word_count > (max(gaps_seen*self.w, self.w)):\n",
    "                if boundaries[gaps_seen] == 1:\n",
    "                    #find closest paragraph break\n",
    "                    best_fit = len(text)\n",
    "                    for br in paragraph_breaks:\n",
    "                        if best_fit > abs(br-char_count):\n",
    "                            best_fit = abs(br-char_count)\n",
    "                            bestbr = br\n",
    "                        else:\n",
    "                            break\n",
    "                    if bestbr not in norm_boundaries: #avoid duplicates\n",
    "                        norm_boundaries.append(bestbr)\n",
    "                gaps_seen += 1\n",
    "\n",
    "        return norm_boundaries\n",
    "\n",
    "\n",
    "class TokenSequence:\n",
    "    \"\"\" A token list with its original length and its index\n",
    "    Source code from nltk.tokenize \"\"\"\n",
    "    def __init__(self,\n",
    "                 index,\n",
    "                 wrdindex_list,\n",
    "                 original_length=None):\n",
    "        \n",
    "        original_length=original_length or len(wrdindex_list)\n",
    "        self.__dict__.update(locals())\n",
    "        del self.__dict__['self']\n",
    "\n",
    "\n",
    "class TokenTableField(object):\n",
    "    \"\"\" A field in the token table holding parameters for each token, used later in the process\n",
    "    Source code from nltk.tokenize \"\"\"\n",
    "    def __init__(self,\n",
    "                 first_pos,\n",
    "                 ts_occurences,\n",
    "                 total_count=1,\n",
    "                 par_count=1,\n",
    "                 last_par=0,\n",
    "                 last_tok_seq=None):\n",
    "        \n",
    "        self.__dict__.update(locals())\n",
    "        del self.__dict__['self']\n",
    "        \n",
    "\n",
    "texttile = HearstTextTiler()\n",
    "texttile.validate('../data/wiki_50')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
